{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet121 - KHOTAA Diabetic Foot Ulcer Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete\n",
      "PyTorch version: 2.10.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import densenet121, DenseNet121_Weights\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('./')\n",
    "\n",
    "from dataset_loader import SplitFolderDatasetLoader\n",
    "from dataset_preprocessing import DFUPreprocessing\n",
    "from utils.checkpoint_manager import CheckpointManager\n",
    "from utils.training_engine import TrainingEngine, create_optimizer\n",
    "from utils.metrics_evaluator import (\n",
    "    calculate_metrics, print_metrics, plot_confusion_matrix,\n",
    "    plot_roc_curve, plot_training_history\n",
    ")\n",
    "\n",
    "print(\"Imports complete\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DatasetLoader] Root: c:\\Users\\sara2\\Documents\\KHOTAA\\dataset\n",
      "[DatasetLoader] Splits: ['train', 'valid', 'test']\n",
      "[DatasetLoader] Classes (4): ['Grade 1', 'Grade 2', 'Grade 3', 'Grade 4']\n",
      "Classes: ['Grade 1', 'Grade 2', 'Grade 3', 'Grade 4']\n",
      "Number of classes: 4\n",
      "[DFUPreprocessing] Initialized\n",
      "[DFUPreprocessing] Image size: 224x224\n",
      "[DFUPreprocessing] Train: with augmentation\n",
      "[DFUPreprocessing] Valid/Test: no augmentation\n",
      "[DatasetLoader] Split 'train': 9639 images\n",
      "[DatasetLoader] Split 'valid': 282 images\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (9639,) (282,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m X_train, y_train = loader.load_split_paths(\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     33\u001b[39m X_val, y_val = loader.load_split_paths(\u001b[33m'\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m X_all = \u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\n\u001b[32m     35\u001b[39m y_all = np.concatenate([y_train, y_val])\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Test set (untouched until final evaluation)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (9639,) (282,) "
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "loader = SplitFolderDatasetLoader(root_dir='../../dataset')\n",
    "classes = loader.get_classes()\n",
    "num_classes = loader.get_num_classes()\n",
    "\n",
    "print(f\"Classes: {classes}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Initialize preprocessing\n",
    "preprocessor = DFUPreprocessing()\n",
    "train_transform = preprocessor.get_train_transforms()\n",
    "val_test_transform = preprocessor.get_valid_test_transforms()\n",
    "\n",
    "# Dataset class\n",
    "class DFUDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        from PIL import Image\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Prepare data for cross-validation\n",
    "X_train, y_train = loader.load_split_paths('train', shuffle=True)\n",
    "X_val, y_val = loader.load_split_paths('valid')\n",
    "X_all = list(X_train) + list(X_val)\n",
    "y_all = np.concatenate([y_train, y_val])\n",
    "\n",
    "# Test set (untouched until final evaluation)\n",
    "X_test, y_test = loader.load_split_paths('test')\n",
    "test_dataset = DFUDataset(X_test, y_test, transform=val_test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# Initialize 5-fold stratified cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"\\nTotal training samples (train+valid): {len(X_all)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(\"Dataset loaded and ready for 5-fold cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DFUPreprocessing] Initialized\n",
      "[DFUPreprocessing] Image size: 224x224\n",
      "[DFUPreprocessing] Train: with augmentation\n",
      "[DFUPreprocessing] Valid/Test: no augmentation\n",
      "✓ Preprocessing Initialized\n",
      "  Training transforms: Augmentation enabled\n",
      "  Validation/Test transforms: No augmentation\n"
     ]
    }
   ],
   "source": [
    "# Setup device and loss function\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Create DenseNet121 model\n",
    "def create_densenet_model(num_classes=4, pretrained=True):\n",
    "    \"\"\"\n",
    "    Create DenseNet121 model for DFU classification.\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of output classes (4 for DFU grades)\n",
    "        pretrained: Use ImageNet pretrained weights\n",
    "    \n",
    "    Returns:\n",
    "        DenseNet121 model configured for DFU classification\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        model = models.densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        model = models.densenet121(weights=None)\n",
    "    \n",
    "    # Modify final classifier layer\n",
    "    # DenseNet uses 'classifier' instead of 'fc'\n",
    "    # DenseNet classifier is: Linear(1024 -> 1000)\n",
    "    # Replace with: Linear(1024 -> num_classes)\n",
    "    model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test model creation\n",
    "test_model = create_densenet_model(num_classes=num_classes)\n",
    "print(f\"\\nDenseNet121 model created\")\n",
    "print(f\"Input size: 224x224\")\n",
    "print(f\"Output classes: {num_classes}\")\n",
    "print(f\"Final classifier: {test_model.classifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DFUDataset class defined\n"
     ]
    }
   ],
   "source": [
    "# 5-Fold Cross-Validation Training\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_all, y_all), 1):\n",
    "    print(f\"\\n{'='*60}\\nFOLD {fold}/5\\n{'='*60}\")\n",
    "    \n",
    "    # Prepare fold data\n",
    "    X_train_fold = [X_all[i] for i in train_idx]\n",
    "    y_train_fold = y_all[train_idx]\n",
    "    X_val_fold = [X_all[i] for i in val_idx]\n",
    "    y_val_fold = y_all[val_idx]\n",
    "    \n",
    "    train_dataset = DFUDataset(X_train_fold, y_train_fold, transform=train_transform)\n",
    "    val_dataset = DFUDataset(X_val_fold, y_val_fold, transform=val_test_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_densenet_model(num_classes=num_classes, pretrained=True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Setup optimizer using helper function (SGD with momentum=0.8)\n",
    "    optimizer = create_optimizer(model, lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    checkpoint_manager = CheckpointManager(base_dir='runs', experiment_name=f'densenet121_fold{fold}')\n",
    "    engine = TrainingEngine(model=model, device=device)\n",
    "    \n",
    "    # Train\n",
    "    history = engine.train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=30,\n",
    "        scheduler=scheduler,\n",
    "        checkpoint_manager=checkpoint_manager,\n",
    "        early_stopping_patience=7,\n",
    "        use_early_stopping=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    best_val_acc = max(history['val_acc'])\n",
    "    fold_results.append({\n",
    "        'fold': fold,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'final_val_acc': history['val_acc'][-1],\n",
    "        'stopped_epoch': history['stopped_epoch'],\n",
    "        'history': history,\n",
    "        'checkpoint_manager': checkpoint_manager\n",
    "    })\n",
    "    print(f\"Fold {fold} - Best Acc: {best_val_acc*100:.2f}% (stopped at epoch {history['stopped_epoch']})\")\n",
    "\n",
    "# Cross-validation summary\n",
    "avg_acc = np.mean([r['best_val_acc'] for r in fold_results])\n",
    "std_acc = np.std([r['best_val_acc'] for r in fold_results])\n",
    "avg_epochs = np.mean([r['stopped_epoch'] for r in fold_results])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"5-FOLD CROSS-VALIDATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mean Accuracy: {avg_acc*100:.2f}% ± {std_acc*100:.2f}%\")\n",
    "print(f\"Average Epochs: {avg_epochs:.1f}\")\n",
    "print(f\"\\nIndividual Fold Results:\")\n",
    "for r in fold_results:\n",
    "    print(f\"  Fold {r['fold']}: {r['best_val_acc']*100:.2f}% (epoch {r['stopped_epoch']})\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DatasetLoader] Split 'train': 9639 images\n",
      "[DatasetLoader] Split 'valid': 282 images\n",
      "✓ Data prepared for CV: 9921 total samples\n",
      "[DatasetLoader] Split 'test': 141 images\n",
      "✓ Test set prepared: 141 test samples\n",
      "✓ 5-Fold Stratified Cross-Validation initialized\n"
     ]
    }
   ],
   "source": [
    "# Test Set Evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load best fold model\n",
    "best_fold_idx = np.argmax([r['best_val_acc'] for r in fold_results])\n",
    "best_fold_num = fold_results[best_fold_idx]['fold']\n",
    "\n",
    "print(f\"Loading best model from Fold {best_fold_num}\")\n",
    "\n",
    "model = create_densenet_model(num_classes=num_classes, pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "checkpoint_manager = fold_results[best_fold_idx]['checkpoint_manager']\n",
    "checkpoint_manager.load_best_model(fold_index=0, create_model_fn=lambda: create_densenet_model(num_classes=num_classes, pretrained=False), metric_name='val_acc')\n",
    "model = model.to(device)\n",
    "\n",
    "engine = TrainingEngine(model=model, device=device)\n",
    "\n",
    "# Evaluate with inference time tracking\n",
    "test_loss, test_acc, predictions, true_labels, inference_time = engine.evaluate(\n",
    "    test_loader, \n",
    "    criterion, \n",
    "    measure_inference_time=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"\\nInference Time Statistics:\")\n",
    "print(f\"  Total Time: {inference_time['total_time']:.4f}s\")\n",
    "print(f\"  Avg Time/Batch: {inference_time['avg_time_per_batch']*1000:.2f}ms ± {inference_time['std_time_per_batch']*1000:.2f}ms\")\n",
    "print(f\"  Avg Time/Image: {inference_time['avg_time_per_image']*1000:.2f}ms\")\n",
    "print(f\"  Throughput: {inference_time['images_per_second']:.1f} images/second\")\n",
    "\n",
    "# Get probabilities for AUC\n",
    "model.eval()\n",
    "all_probs = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs.to(device))\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "y_pred_proba = np.vstack(all_probs)\n",
    "\n",
    "# Calculate all metrics\n",
    "metrics = calculate_metrics(\n",
    "    y_true=true_labels,\n",
    "    y_pred=predictions,\n",
    "    y_pred_proba=y_pred_proba,\n",
    "    class_names=classes,\n",
    "    average='macro'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print_metrics(metrics, title=\"DenseNet121 Test Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualizations\n",
    "import os\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Confusion Matrix\n",
    "plot_confusion_matrix(\n",
    "    y_true=true_labels,\n",
    "    y_pred=predictions,\n",
    "    class_names=classes,\n",
    "    normalize=True,\n",
    "    save_path='results/densenet121_confusion_matrix.png'\n",
    ")\n",
    "print(\"\\nConfusion matrix saved to results/densenet121_confusion_matrix.png\")\n",
    "\n",
    "# ROC Curve\n",
    "plot_roc_curve(\n",
    "    y_true=true_labels,\n",
    "    y_pred_proba=y_pred_proba,\n",
    "    class_names=classes,\n",
    "    save_path='results/densenet121_roc_curve.png'\n",
    ")\n",
    "print(\"ROC curve saved to results/densenet121_roc_curve.png\")\n",
    "\n",
    "# Training History (best fold)\n",
    "plot_training_history(\n",
    "    fold_results[best_fold_idx]['history'],\n",
    "    save_path='results/densenet121_training_history.png'\n",
    ")\n",
    "print(\"Training history saved to results/densenet121_training_history.png\")\n",
    "\n",
    "# Summary for Model Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY FOR MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: DenseNet121\")\n",
    "print(f\"Cross-Validation Accuracy: {avg_acc*100:.2f}% ± {std_acc*100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test F1-Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"Test MCC: {metrics['mcc']:.4f}\")\n",
    "print(f\"Test AUC: {metrics['auc']:.4f}\")\n",
    "print(f\"Average Training Epochs: {avg_epochs:.1f}\")\n",
    "print(f\"Inference Time: {inference_time['avg_time_per_image']*1000:.2f}ms per image\")\n",
    "print(f\"Throughput: {inference_time['images_per_second']:.1f} images/second\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results for Model Comparison\n",
    "\n",
    "Save the results for later comparison with other models (ResNet50, ResNet101, MobileNet, GoogLeNet, EfficientNetV2S, PFCNN+DRNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Device: cpu\n",
      "✓ Loss function: CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "# Save results for model comparison\n",
    "import json\n",
    "\n",
    "densenet_results = {\n",
    "    'model_name': 'DenseNet121',\n",
    "    'cv_results': {\n",
    "        'val_accuracy': {'mean': float(avg_acc), 'std': float(std_acc)},\n",
    "        'avg_epochs': float(avg_epochs),\n",
    "        'fold_results': [\n",
    "            {\n",
    "                'fold': r['fold'],\n",
    "                'best_val_acc': float(r['best_val_acc']),\n",
    "                'stopped_epoch': int(r['stopped_epoch'])\n",
    "            }\n",
    "            for r in fold_results\n",
    "        ]\n",
    "    },\n",
    "    'test_results': {\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_loss': float(test_loss),\n",
    "        'precision': float(metrics['precision']),\n",
    "        'recall': float(metrics['recall']),\n",
    "        'f1_score': float(metrics['f1_score']),\n",
    "        'specificity': float(metrics['specificity']),\n",
    "        'sensitivity': float(metrics['sensitivity']),\n",
    "        'mcc': float(metrics['mcc']),\n",
    "        'auc': float(metrics['auc'])\n",
    "    },\n",
    "    'inference_time': {\n",
    "        'total_time': float(inference_time['total_time']),\n",
    "        'avg_time_per_image_ms': float(inference_time['avg_time_per_image'] * 1000),\n",
    "        'throughput_fps': float(inference_time['images_per_second'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "os.makedirs('results', exist_ok=True)\n",
    "with open('results/densenet121_results.json', 'w') as f:\n",
    "    json.dump(densenet_results, f, indent=4)\n",
    "\n",
    "print(\"Results saved to results/densenet121_results.json\")\n",
    "print(\"\\nThese results can be used with the ModelComparison utility:\")\n",
    "print(\"from utils.model_comparison import ModelComparison\")\n",
    "print(\"comparison = ModelComparison()\")\n",
    "print(\"comparison.add_model_result(**densenet_results)\")\n",
    "print(\"\\nDenseNet121 training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
